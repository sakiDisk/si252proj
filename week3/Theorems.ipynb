{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43e239c9",
   "metadata": {},
   "source": [
    "### **Theorem 1 Statement**:\n",
    "\n",
    "Let $I$ be a CMAB-T problem and $D$ a dataset with $n$ data samples. The action $Ŝ$ returned by the CLCB algorithm (using an $α$-approximate oracle) satisfies:\n",
    "\n",
    "* Monotonicity condition, ensures that the reward function is monotonically increasing in $\\mu_i$. This condition ensures that any improvement in the base arms' means leads to an improvement in the overall reward of the super arm.\n",
    "* 1-norm TPM smoothness condition with coefficient $B1$, bounds the sensitivity of the reward with respect to changes in the base arm's means. This condition helps to control how much the reward function changes when individual base arms are modified.\n",
    "* Infinity-norm TPM data coverage condition with coefficient $C∗_\\infty$, quantifies the amount of data required to accurately estimate the reward for each base arm. It ensures that the dataset is sufficient for making reliable estimates, which is essential for achieving tight bounds on the suboptimality gap.\n",
    "  if the number of samples satisfies:\n",
    "$$\n",
    "n ≥ 8 \\log \\left(\\frac{m}{\\delta}\\right) \\min_{i \\in [m]: p_{Darm, S^∗_i} > 0} \\frac{1}{p_{Darm, DS_i}},\n",
    "$$\n",
    "then, with probability at least $1 - \\delta$, the suboptimality gap satisfies:\n",
    "$$\n",
    "\\text{SubOpt}(\\hat{S}; \\alpha, I) \\leq 2 \\alpha B1 \\cdot \\overline{K}^∗_2 \\cdot \\sqrt{2 C∗_\\infty \\log \\left( \\frac{2mn}{\\delta} \\right) / n},\n",
    "$$\n",
    "\n",
    "where $\\overline{K}^∗_2 = \\sum_{i \\in [m]} \\sqrt{p_{Darm, S^∗_i}}$ is the $ℓ_2$ action size of the optimal action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7530e95d",
   "metadata": {},
   "source": [
    "### **Proof**:\n",
    "\n",
    "* **Suboptimality Gap Decomposition**: The suboptimality gap is the difference in rewards between the optimal action $S^∗$ and the action $\\hat{S}$ selected by the CLCB algorithm. We express this gap in terms of the uncertainty gap, oracle gap, and pessimism gap.\n",
    "$$ \\text{SubOpt}(Ŝ; \\alpha, I) = \\alpha \\left( r(S^∗; \\mu) - r(Ŝ; \\mu) \\right) $$\n",
    "\n",
    "* ** Uncertainty Gap**: The uncertainty gap measures the difference between the reward of the optimal action and the reward computed using the lower confidence bounds (LCBs) of the base arms. We use **monotonicity (Condition 1)** and the **1-norm TPM smoothness condition (Condition 2)** to relate the uncertainty gap to the per-arm estimation gap.\n",
    "$$r(S^∗; \\mu) - r(S^∗; \\hat{\\mu}) \\leq \\sum_{i \\in [m]} p_{Darm, S^∗_i} \\left( \\mu_i - \\hat{\\mu}_i \\right)$$\n",
    "\n",
    "* ** Oracle Gap**: The oracle gap is the difference in rewards between the optimal action and the action chosen by the oracle based on the LCB estimates. Since the oracle approximates the best possible action under the LCB estimates, this gap is minimized. The oracle gap is bounded using the **smoothness condition (Condition 2)**.\n",
    "\n",
    "* **Pessimism Gap**: The pessimism gap is due to the pessimistic nature of the CLCB algorithm, which penalizes base arms with high uncertainty. The algorithm uses the LCB estimates to avoid selecting actions with high uncertainty. We use the **infinity-norm data coverage condition (Condition 3)** to bound the pessimism gap.\n",
    "\n",
    "$$\\boxed{\\text{SubOpt}(\\hat{S}; \\alpha, I) \\leq 2 \\alpha B1 \\cdot \\sum_{i \\in [m]} \\sqrt{p_{Darm, S^∗_i}} \\cdot \\sqrt{\\frac{2 \\log \\left( \\frac{2mn}{\\delta} \\right)}{n}} \\cdot C∗_\\infty}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c86f70",
   "metadata": {},
   "source": [
    "### **boundary conditions** : $1 - \\delta$\n",
    "1. **$\\delta$**:  **failure probability**. It quantifies the likelihood that the algorithm’s performance will **not** meet the theoretical bound (i.e., the suboptimality gap being small).\n",
    "\n",
    "2. The theorem guarantees that the suboptimality gap of the algorithm will be bounded as described **with probability $1 - \\delta$**. This means that there is a $1 - \\delta$ probability that the actual gap will be at most the value provided in the theorem (i.e., it will be \"close to optimal\").\n",
    "\n",
    "3. The number $1 - \\delta$ represents the confidence level, and it comes from the **probability bounds** derived from concentration inequalities (e.g., **Hoeffding’s inequality**). These inequalities provide guarantees that, as the number of samples $n$ increases, the estimated values (such as the mean reward) will be close to their true values with high probability. In the context of the proof, the algorithm uses these statistical tools to bound the uncertainty in the estimates of the reward for each action. The **$1 - \\delta$** bound ensures that the uncertainty is controlled with high confidence (1 - $\\delta$).\n",
    "\n",
    "\n",
    "#### Note:  **Hoeffding's Inequality**:\n",
    "A common approach to establish such bounds is through Hoeffding's inequality, which provides a concentration bound for the sum of bounded random variables. In the case of the CLCB algorithm, this inequality would ensure that the empirical means $\\hat{\\mu}_i$ for each base arm $i$ are close to their true means $\\mu_i$ with high probability. By applying Hoeffding’s inequality, we can show that the probability of an individual empirical mean being far from the true mean is less than $\\delta$. Therefore, with **probability $1 - \\delta$**, the estimates will be sufficiently close to the true values, ensuring that the suboptimality gap is small.\n",
    "\n",
    "The **probability** $1 - \\delta$ is a result of these statistical bounds, ensuring that the algorithm’s performance meets the bound with high likelihood (i.e., it works well most of the time, except for a small fraction of cases, which is controlled by $\\delta$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baecfda",
   "metadata": {},
   "source": [
    "**Suboptimality Gap**: Defined as the difference in expected rewards between the optimal action $S^*$ and the selected action $\\hat{S}$:\n",
    "$$\n",
    "\\text{SubOpt}(\\hat{S}; \\alpha, I) = \\alpha \\cdot r(S^*; \\mu) - r(\\hat{S}; \\mu)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $r(S^*; \\mu)$ is the expected reward of the optimal action $S^*$,\n",
    "* $r(\\hat{S}; \\mu)$ is the expected reward of the chosen action $\\hat{S}$,\n",
    "* $\\alpha$ is the approximation factor from the oracle, and\n",
    "* $\\mu$ represents the true mean vector.\n",
    "\n",
    "**Decomposition of the Suboptimality Gap**: We decompose the suboptimality gap into three terms:\n",
    "1. **Uncertainty gap**: the difference between the true reward $r(S^*; \\mu)$ and its estimate $r(S^*; \\hat{\\mu})$.\n",
    "2. **Oracle gap**: the difference between the optimal reward $r(S^*; \\hat{\\mu})$ and the reward $r(\\hat{S}; \\hat{\\mu})$ obtained by the oracle.\n",
    "3. **Pessimism gap**: the difference between $r(\\hat{S}; \\hat{\\mu})$ and the reward of the selected action $r(\\hat{S}; \\mu)$.\n",
    "\n",
    "**Upper Bound for the Suboptimality Gap**\n",
    "$$\n",
    "\\text{SubOpt}(\\hat{S}; \\alpha, I) \\leq \\alpha \\left( r(S^*; \\mu) - r(S^*; \\hat{\\mu}) \\right) + \\left( r(S^*; \\hat{\\mu}) - r(\\hat{S}; \\hat{\\mu}) \\right) + \\left( r(\\hat{S}; \\hat{\\mu}) - r(\\hat{S}; \\mu) \\right)\n",
    "$$\n",
    "\n",
    "* **a: Uncertainty Gap**: This term is bounded by\n",
    "$$\n",
    "r(S^*; \\mu) - r(S^*; \\hat{\\mu}) \\leq B_1 \\sum_{i \\in [m]} p_{Darm, S^*} \\left( \\mu_i - \\hat{\\mu}_i \\right)\n",
    "$$\n",
    "Where $B_1$ is the smoothness coefficient and $p_{Darm, S^*}$ represents the data-triggered probability for each base arm.\n",
    "* **b: Oracle Gap**: The oracle gap is controlled by the error of the oracle. Since the oracle returns an $\\alpha$-approximation of the optimal arm, we can ensure that:\n",
    "$$r(S^*; \\hat{\\mu}) - r(\\hat{S}; \\hat{\\mu}) \\leq 0$$\n",
    "* **c: Pessimism Gap**: This gap is controlled by the LCB (Lower Confidence Bound) of each base arm. By ensuring that the confidence bounds for each arm are sufficiently tight, we can bound this gap:\n",
    "$$r(\\hat{S}; \\hat{\\mu}) - r(\\hat{S}; \\mu) \\leq \\sqrt{\\frac{\\log(4mn/\\delta)}{2N_i}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c7b7a",
   "metadata": {},
   "source": [
    "#### PreProbs:\n",
    "1. **k-path Problem**: We have $m$ arms, where $m/k$ is an integer, and the set of possible combinatorial actions consists of $m/k$ paths, each containing $k$ unique arms. The reward structure for these arms is defined such that arms within each path are dependent, and their outcomes are Bernoulli random variables with the same mean.\n",
    "2. **Data Coverage Condition**: The dataset used for learning must satisfy the **infinity-norm TPM data coverage condition** (Condition 3), which ensures that the collected dataset provides a sufficient number of samples for each arm in the optimal action. Specifically, for each arm, the ratio of the data collection probability to the triggering probability must be bounded by a constant.\n",
    "\n",
    "#### The Lower Bound: The goal is to show that, for any algorithm $A$ returning an action $\\hat S$ from the dataset $D$, the **suboptimality gap** (i.e., the difference between the reward of the optimal action $S^*$ and the reward of the selected action $\\hat S$) cannot be arbitrarily small, even for an optimal algorithm. This lower bound is derived using the following steps:\n",
    "1. **Gap Definition**: We define the suboptimality gap for an action $\\hat S$ as:\n",
    "   $$\n",
    "   g(Ŝ; \\mu) = r(S^*; \\mu) - r(Ŝ; \\mu)\n",
    "   $$\n",
    "   where $r(S; \\mu)$ represents the expected reward of an action $S$ given the mean vector $\\mu$.\n",
    "2. **Two Problem Instances**: The lower bound is derived by comparing two problem instances, `P1` and `P2`, that differ only in a small gap between the means of the arms in the paths. Specifically, the mean vector for the arms in the first path of `P1` is slightly smaller than that of `P2`, creating a gap that the algorithm must learn to distinguish.\n",
    "3. **Key Inequality (Le Cam's method)**: Using Le Cam's method, we relate the gap between the optimal action and the selected action in terms of the **Kullback-Leibler divergence (KL divergence)** between the two problem instances. This divergence quantifies the difficulty of distinguishing between the two instances based on the dataset, and it leads to the lower bound for the gap:\n",
    "   $$\n",
    "   \\inf_A \\sup_{(Darm, DS) \\in \\text{k-path}(m, k, C^*_\\infty)} \\mathbb{E}_D \\left[ r(S^*; \\mu) - r(A(D); \\mu) \\right] \\geq \\min \\left( 1, \\frac{\\sqrt{C^*_\\infty}}{n} \\right)\n",
    "   $$\n",
    "4. **Lower Bound Expression**: The final lower bound on the suboptimality gap is given by:\n",
    "   $$\n",
    "   \\inf_A \\sup_{(Darm, DS) \\in \\text{k-path}(m, k, C^*_\\infty)} \\mathbb{E}_D \\left[ r(S^*; \\mu) - r(A(D); \\mu) \\right] \\geq k \\cdot \\min \\left( 1, \\frac{\\sqrt{C^*_\\infty}}{n} \\right)\n",
    "   $$\n",
    "This result establishes that, regardless of the algorithm used, the suboptimality gap cannot be smaller than this lower bound, which depends on the action size $k$, the data coverage coefficient $C^*_\\infty$, and the number of samples $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2313444a",
   "metadata": {},
   "source": [
    "### Theorem 2: Lower Bound for Suboptimality Gap\n",
    "\n",
    "The theorem states that for any algorithm $A$ applied to a dataset $D$ of size $n$, the suboptimality gap between the action chosen by the algorithm and the optimal action satisfies the following lower bound:\n",
    "$$\n",
    "\\inf_A \\sup_{(D_{arm}, D_{DS}) \\in \\text{k-path}(m, k, C_\\infty^*)} \\mathbb{E}[r(S^*; \\mu) - r(A(D); \\mu)] \\geq k \\min\\left(1, \\sqrt{\\frac{C_\\infty^*}{n}}\\right)\n",
    "$$\n",
    "Where:\n",
    "* $S^*$ is the optimal action.\n",
    "* $\\mu$ is the mean reward vector.\n",
    "* $C_\\infty^*$ is the coverage coefficient for the infinity-norm data condition.\n",
    "\n",
    "\n",
    "The suboptimality gap $g(S)$ for any action $S$ is defined as:\n",
    "   $$\n",
    "   g(S; \\mu) := r(S^*; \\mu) - r(S; \\mu)\n",
    "   $$\n",
    "   where $S^*$ is the optimal action.\n",
    "\n",
    "We consider two problem instances $P_1$ and $P_2$, both following the k-path problem structure, but with slight variations in their reward distributions. Specifically:\n",
    "   * The first instance $P_1$ has a reward vector $\\mu_1 = \\left( \\frac{1}{2}, \\frac{1}{2} - \\Delta, 0, \\dots, 0 \\right)$.\n",
    "   * The second instance $P_2$ has a reward vector $\\mu_2 = \\left( \\frac{1}{2}, \\frac{1}{2} + \\Delta, 0, \\dots, 0 \\right)$.\n",
    "\n",
    "The two instances $P_1$ and $P_2$ satisfy the infinity-norm TPM data coverage condition (Condition 3). This ensures that the worst-case data coverage is bounded by $C_\\infty^*$.\n",
    "\n",
    "The proof leverages Le Cam’s method, which allows us to relate the expected suboptimality gap to the Kullback-Leibler (KL) divergence between the two problem instances. Specifically, the KL divergence between the two reward distributions $\\mu_1$ and $\\mu_2$ is used to lower bound the gap.\n",
    "\n",
    "   After applying the necessary bounds on the divergence, we obtain the lower bound for the suboptimality gap as:\n",
    "   $$\n",
    "   g(S; \\mu) \\geq k \\min\\left( 1, \\sqrt{\\frac{C_\\infty^*}{n}} \\right)\n",
    "   $$\n",
    "   This result establishes that the gap grows with $k$ and diminishes with $n$, the number of samples, as expected for offline learning.\n",
    "\n",
    "This lower bound is useful for comparing the performance of algorithms in offline combinatorial multi-armed bandit problems, showing the inherent difficulty of finding an optimal action and the relationship between data coverage and sample size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:c182]",
   "language": "python",
   "name": "conda-env-c182-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
