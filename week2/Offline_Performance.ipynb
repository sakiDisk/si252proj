{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94472965",
   "metadata": {},
   "source": [
    "This part describes the offline data collection process for the combinatorial multi-armed bandit with probabilistically triggered arms (CMAB-T) framework. The dataset consists of feedback data, where each data sample includes a combinatorial action, a triggered set of base arms, and their corresponding outcomes. The dataset is pre-collected by an experimenter, with each sample generated independently from a distribution over feasible actions and feedback. The experimenter’s data collection distribution is assumed to be unknown, and it is important to capture the frequency of observing each arm and its associated outcomes. The paper also introduces the data-triggering probability, which indicates the frequency of each arm being observed during the data collection process.\n",
    "\n",
    "The performance of an offline learning algorithm in this framework is evaluated based on the **suboptimality gap**. This gap quantifies the difference in expected reward between the optimal action and the action chosen by the algorithm, under the assumption that the algorithm has access to an approximation oracle. The oracle helps approximate the optimal solution by providing a near-optimal action. The authors define the **α-approximate suboptimality gap**, which is used to measure the performance of the offline learning algorithm. The objective is to minimize this gap, ensuring that the selected action performs close to the optimal one with high probability.\n",
    "\n",
    "The section further introduces the concept of **data coverage conditions**, which are used to assess the quality of the offline dataset. These conditions determine the required data coverage for accurately estimating the reward of the optimal action. Two conditions are proposed: the **infinity-norm** and the **1-norm** triggering probability modulated (TPM) data coverage conditions. These conditions help ensure that the dataset provides sufficient information to estimate the expected reward of the optimal action with a small suboptimality gap. The quality of the dataset plays a critical role in the efficiency of offline learning, as the better the coverage, the fewer samples are needed to achieve a near-optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb439c63",
   "metadata": {},
   "source": [
    "### Related Formulas:\n",
    "\n",
    "   * Suboptimality Gap:\n",
    "\n",
    "     $$\n",
    "     \\text{SubOpt}(Ŝ; \\alpha, I) := \\alpha \\cdot r(S^*; \\mu) - r(Ŝ; \\mu)\n",
    "     $$\n",
    "\n",
    "     where $r(S^*; \\mu)$ is the expected reward of the optimal action $S^*$, and $r(Ŝ; \\mu)$ is the reward of the chosen action $Ŝ$.\n",
    "\n",
    "   * Data Triggering Probability:\n",
    "\n",
    "     $$\n",
    "     p_{Darm,DS}^i = \\mathbb{E}_{S \\sim DS, X \\sim Darm, \\tau \\sim D_{trig}(S, X)}[I(i \\in \\tau)]\n",
    "     $$\n",
    "\n",
    "     This represents the frequency of observing base arm $i$ when selecting action $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3391e8e",
   "metadata": {},
   "source": [
    "### Pseudocode for Offline Learning Algorithm:\n",
    "\n",
    "   ```python\n",
    "   # Input: Dataset D, α-approximation oracle ORACLE, failure probability δ\n",
    "   # Output: Chosen combinatorial action Ŝ\n",
    "\n",
    "   for arm i in [m]:\n",
    "       # Calculate number of times arm i was observed\n",
    "       Ni = sum([1 for t in range(n) if i in τt])\n",
    "       # Calculate empirical mean for arm i\n",
    "       mu_hat_i = sum([Xt,i for t in range(n) if i in τt]) / Ni\n",
    "       # Compute lower confidence bound (LCB) for arm i\n",
    "       LCB(µi) = mu_hat_i - sqrt(log(4mn/δ) / (2Ni))\n",
    "\n",
    "   # Use oracle to find the best combinatorial action based on LCBs\n",
    "   S_hat = ORACLE(LCB(µ1), ..., LCB(µm))\n",
    "   return S_hat\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d835fa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected best action is arm 2 (0-based index).\n"
     ]
    }
   ],
   "source": [
    "# Create a sample dataset \n",
    "# Define a simple oracle that selects the action with the maximum LCB.\n",
    "import numpy as np\n",
    "\n",
    "# Function to simulate a dataset for arms, where each arm has n outcomes\n",
    "def generate_dataset(m, n):\n",
    "    \"\"\"Generate a dataset for m arms, each having n outcomes drawn from a uniform distribution [0, 1].\"\"\"\n",
    "    return [np.random.uniform(0, 1, n) for _ in range(m)]\n",
    "\n",
    "# Define the oracle function that selects the action with the maximum LCB\n",
    "def simple_oracle(LCBs):\n",
    "    \"\"\"Oracle that selects the action (arm) with the maximum LCB.\"\"\"\n",
    "    return np.argmax(LCBs)\n",
    "\n",
    "# Compute lower confidence bounds (LCBs)\n",
    "def compute_LCB(data, delta, n):\n",
    "    \"\"\"Compute the LCBs for each arm based on the dataset.\"\"\"\n",
    "    m = len(data)  # Number of arms\n",
    "    LCB = []\n",
    "    for arm_data in data:\n",
    "        Ni = len(arm_data)\n",
    "        mean = np.mean(arm_data)\n",
    "        lcb = mean - np.sqrt(np.log(4 * m * n / delta) / (2 * Ni))\n",
    "        LCB.append(lcb)\n",
    "    return LCB\n",
    "\n",
    "# The CLCB algorithm implementation\n",
    "def CLCB_algorithm(dataset, oracle, delta, n):\n",
    "    \"\"\"The CLCB algorithm to select the best action based on LCBs.\"\"\"\n",
    "    # Compute LCBs for all arms\n",
    "    LCBs = compute_LCB(dataset, delta, n)\n",
    "    \n",
    "    # Use the oracle to select the best action based on LCBs\n",
    "    S_hat = oracle(LCBs)\n",
    "    \n",
    "    return S_hat\n",
    "\n",
    "# Parameters\n",
    "m = 5  # Number of arms\n",
    "n = 100  # Number of rounds (samples)\n",
    "delta = 0.1  # Failure probability\n",
    "\n",
    "# Generate a sample dataset of m arms and n samples for each arm\n",
    "dataset = generate_dataset(m, n)\n",
    "\n",
    "# Run the CLCB algorithm to select the best action\n",
    "best_action = CLCB_algorithm(dataset, simple_oracle, delta, n)\n",
    "\n",
    "print(f\"The selected best action is arm {best_action + 1} (0-based index).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc561fa",
   "metadata": {},
   "source": [
    "1. **Dataset**: We generate a dataset with `m` arms, and each arm has `n` samples (outcomes). For simplicity, each outcome is drawn randomly from a uniform distribution between 0 and 1.\n",
    "2. **Oracle**: The `simple_oracle` function selects the action with the highest LCB from the list of LCBs computed for each arm.\n",
    "3. **CLCB\\_algorithm**: This is the main function that calculates the LCBs for each arm and uses the oracle to select the best arm.\n",
    "4. **Parameters**:\n",
    "   * `m` (5): Number of arms.\n",
    "   * `n` (100): Number of rounds or samples per arm.\n",
    "   * `delta` (0.1): The failure probability used in the LCB calculation.\n",
    "\n",
    "* The `CLCB_algorithm` will compute the LCB for each arm based on the dataset and select the arm with the highest LCB as the best action.\n",
    "* The output will tell you which arm (indexed from 1) is selected as the best action based on the algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:c182]",
   "language": "python",
   "name": "conda-env-c182-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
