{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a568cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a08bef",
   "metadata": {},
   "source": [
    "### Simple Combinatorial Multi-Armed Bandit (CMAB)\n",
    "\n",
    "1.Explanation:\n",
    "\n",
    "- **Arms and rewards:** Each base arm has a random reward between 0 and 1 (simulated by `true_rewards`).\n",
    "- **UCB:** The `UCB` class implements the Upper Confidence Bound algorithm, where each armâ€™s selection is based on its estimated value and a confidence interval that decreases as more samples are collected.\n",
    "- **Simulation loop:** In each round, a combinatorial set of `k` arms is selected randomly, and their rewards are accumulated. The UCB values for each arm are updated based on these rewards.\n",
    "- **Reward calculation:** A Bernoulli distribution is used to simulate the reward feedback for each arm.\n",
    "\n",
    "2.Output: The output will show the total rewards for each round and the final learned values of each arm after the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aedaeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0: Total Reward 1.00\n",
      "Round 100: Total Reward 2.00\n",
      "Round 200: Total Reward 2.00\n",
      "Round 300: Total Reward 2.00\n",
      "Round 400: Total Reward 0.00\n",
      "Round 500: Total Reward 3.00\n",
      "Round 600: Total Reward 3.00\n",
      "Round 700: Total Reward 3.00\n",
      "Round 800: Total Reward 3.00\n",
      "Round 900: Total Reward 2.00\n",
      "\n",
      "Final estimated values of each arm:\n",
      "[0.33167496 0.48047538 0.82769726 0.73166667 0.49403748]\n"
     ]
    }
   ],
   "source": [
    "n_arms = 5 # Number of base arms\n",
    "k = 3 # Number of arms in each combinatorial action\n",
    "n_rounds = 1000 # Total number of rounds to simulate\n",
    "# Random reward function (between 0 and 1) for each arm\n",
    "true_rewards = np.random.rand(n_arms)\n",
    "\n",
    "# Function to simulate the reward of a selected arm\n",
    "def get_reward(arm_index):\n",
    "    # Simulate Bernoulli feedback with some randomness\n",
    "    return np.random.binomial(1, true_rewards[arm_index])\n",
    "\n",
    "# Upper Confidence Bound (UCB) algorithm for CMAB\n",
    "class UCB:\n",
    "    def __init__(self, n_arms, k):\n",
    "        self.n_arms = n_arms\n",
    "        self.k = k\n",
    "        self.counts = np.zeros(n_arms)  # Counts of selections for each arm\n",
    "        self.values = np.zeros(n_arms)  # Estimated values for each arm\n",
    "        self.total_rewards = np.zeros(n_arms)  # Total rewards collected for each arm\n",
    "        self.time = 0  # Total number of actions taken\n",
    "    def select_action(self):\n",
    "        # UCB formula: Choose arm with the highest UCB\n",
    "        ucb_values = self.values + np.sqrt(2 * np.log(self.time + 1) / (self.counts + 1))\n",
    "        return np.argmax(ucb_values)\n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.time += 1\n",
    "        self.counts[chosen_arm] += 1\n",
    "        self.total_rewards[chosen_arm] += reward\n",
    "        self.values[chosen_arm] = self.total_rewards[chosen_arm] / self.counts[chosen_arm]\n",
    "        \n",
    "# Initialize the UCB algorithm for CMAB\n",
    "ucb = UCB(n_arms, k)\n",
    "# Simulate the CMAB process\n",
    "for round in range(n_rounds):\n",
    "    chosen_arms = random.sample(range(n_arms), k)  # Select k arms (combinatorial choice)\n",
    "    total_reward = 0\n",
    "    # Collect rewards for each arm in the selected combination\n",
    "    for arm in chosen_arms:\n",
    "        reward = get_reward(arm)\n",
    "        ucb.update(arm, reward)\n",
    "        total_reward += reward\n",
    "    # Output results (optional)\n",
    "    if round % 100 == 0:\n",
    "        print(f\"Round {round}: Total Reward {total_reward:.2f}\")\n",
    "\n",
    "# After all rounds, show the learned values\n",
    "print(\"\\nFinal estimated values of each arm:\")\n",
    "print(ucb.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f01549c",
   "metadata": {},
   "source": [
    "### Off-CMAB\n",
    "The **Off-CMAB** framework is an offline version of the Combinatorial Multi-Armed Bandit (CMAB), which works by utilizing pre-collected offline data (a dataset of actions and corresponding feedback) rather than exploring the environment in real-time. The **Combinatorial Lower Confidence Bound (CLCB)** algorithm is commonly used for Off-CMAB. It combines pessimistic estimations of rewards for base arms with a combinatorial solver. Below is the Python code for **Off-CMAB** using the **CLCB algorithm**, assuming we have a pre-collected dataset. The code leverages a given dataset of actions and feedback to select a combinatorial action that maximizes rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ecc580b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final estimated values of each arm (LCB values):\n",
      "[0.11111111 0.94736842 0.921875   0.72307692 0.11764706]\n",
      "\n",
      "Chosen arms for the final action: [3, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# Simulated environment for generating rewards\n",
    "def get_reward(arm_index, true_rewards):\n",
    "    # Simulate Bernoulli feedback (reward = 1 with true_rewards probability)\n",
    "    return np.random.binomial(1, true_rewards[arm_index])\n",
    "\n",
    "# Offline dataset (simulated)\n",
    "def generate_offline_dataset(n_samples, n_arms, k):\n",
    "    dataset = []\n",
    "    for _ in range(n_samples):\n",
    "        chosen_arms = random.sample(range(n_arms), k)  # Randomly select k arms for this sample\n",
    "        rewards = [get_reward(arm, true_rewards) for arm in chosen_arms]  # Get rewards for selected arms\n",
    "        dataset.append((chosen_arms, rewards))\n",
    "    return dataset\n",
    "\n",
    "# Combinatorial Lower Confidence Bound (CLCB) for Off-CMAB\n",
    "class OffCMAB:\n",
    "    def __init__(self, n_arms, k, dataset):\n",
    "        self.n_arms = n_arms\n",
    "        self.k = k\n",
    "        self.dataset = dataset\n",
    "        self.counts = np.zeros(n_arms)  # How many times each arm has been selected\n",
    "        self.total_rewards = np.zeros(n_arms)  # Total rewards for each arm\n",
    "        self.values = np.zeros(n_arms)  # Estimated values (mean rewards) for each arm\n",
    "        self.time = 0  # Total number of actions processed\n",
    "    def compute_lcb(self, arm_index):\n",
    "        # Calculate the lower confidence bound (LCB) for the arm\n",
    "        if self.counts[arm_index] == 0:\n",
    "            return float('inf')  # If arm hasn't been selected, set a high LCB\n",
    "        lcb = self.values[arm_index] - np.sqrt(np.log(self.time + 1) / (2 * self.counts[arm_index]))\n",
    "        return lcb\n",
    "    def select_action(self):\n",
    "        # Select the arm with the lowest LCB\n",
    "        lcb_values = [self.compute_lcb(arm) for arm in range(self.n_arms)]\n",
    "        return np.argmin(lcb_values)\n",
    "    def update(self, chosen_arms, rewards):\n",
    "        # Update the counts, total rewards, and estimated values of the chosen arms\n",
    "        for arm, reward in zip(chosen_arms, rewards):\n",
    "            self.counts[arm] += 1\n",
    "            self.total_rewards[arm] += reward\n",
    "            self.values[arm] = self.total_rewards[arm] / self.counts[arm]\n",
    "        self.time += 1  # Increment the time step\n",
    "    def train(self):\n",
    "        # Train the Off-CMAB using the offline dataset\n",
    "        for chosen_arms, rewards in self.dataset:\n",
    "            self.update(chosen_arms, rewards)\n",
    "\n",
    "# Simulate the true rewards for each arm\n",
    "n_arms = 5\n",
    "true_rewards = np.random.rand(n_arms)  # Simulated true reward for each arm\n",
    "# Simulate offline dataset (using 100 samples)\n",
    "n_samples = 100\n",
    "k = 3  # Number of arms selected in each combinatorial action\n",
    "dataset = generate_offline_dataset(n_samples, n_arms, k)\n",
    "# Initialize Off-CMAB algorithm\n",
    "off_cmab = OffCMAB(n_arms, k, dataset)\n",
    "# Train Off-CMAB on the offline dataset\n",
    "off_cmab.train()\n",
    "# Display the learned values of each arm\n",
    "print(\"Final estimated values of each arm (LCB values):\")\n",
    "print(off_cmab.values)\n",
    "# Select a combinatorial action based on the LCB values\n",
    "chosen_action = random.sample(range(n_arms), k)  # For simplicity, select a random action from the arms\n",
    "print(f\"\\nChosen arms for the final action: {chosen_action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a461ae",
   "metadata": {},
   "source": [
    "1. Explanation:\n",
    "\n",
    "- **Offline Dataset Generation**: The `generate_offline_dataset()` function simulates an offline dataset with `n_samples` entries. For each sample, `k` arms are randomly chosen, and their rewards are generated based on `true_rewards`.\n",
    "\n",
    "- **Off-CMAB** :\n",
    "\n",
    "   * **Initialization**: The class stores the number of arms (`n_arms`), the number of arms selected per action (`k`), and the offline dataset.\n",
    "   * **LCB Calculation**: The `compute_lcb()` method computes the lower confidence bound (LCB) for a given arm, based on the number of times it has been selected and its observed rewards.\n",
    "   * **Action Selection**: The `select_action()` method selects the arm with the smallest LCB, which corresponds to the arm that has the least uncertainty in terms of expected reward.\n",
    "   * **Update**: The `update()` method updates the arm statistics after each sample in the dataset is processed.\n",
    "\n",
    "- **Training**: The `train()` method trains the algorithm using the offline dataset, iterating through each sample and updating the arm statistics based on the observed rewards.\n",
    "\n",
    "- **Simulated Rewards**: The `true_rewards` array holds the true reward probability for each arm. The rewards are simulated using a Bernoulli distribution, where each arm's reward is a success with probability equal to its corresponding true reward.\n",
    "\n",
    "- **Final Action Selection**: After training, the estimated values for each arm are printed, and a final action (combinatorial selection of `k` arms) is made based on these values.\n",
    "\n",
    "2. Output: The output will display the learned values for each arm after training (based on the offline dataset) and the final selection of `k` arms based on these values.\n",
    "\n",
    "3. Note: This is only a demo, for it has a simple reward model and small datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9565bdb8",
   "metadata": {},
   "source": [
    "### Cascading bandits (CB)\n",
    "\n",
    "Cascading Bandits is a problem where you have a sequence of items (such as a list of recommendations) and a user interacts with them in a sequential manner. In each round, the user examines the items in the order presented, and once they find a satisfactory item, they stop interacting with the remaining items. This feedback is referred to as cascading feedback. The goal of the bandit algorithm is to learn the best ordering of items (the \"ranked list\") to maximize user engagement. The following is model of `Cascading Bandits`: \n",
    "1. **Sequential Decision Making**: The learner will recommend a list of items to a user.\n",
    "2. **Cascading Feedback**: The feedback is observed based on the user stopping at the first satisfactory item.\n",
    "\n",
    "Note: Like `CMAB`, this part we still use `UCB` algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e55bfb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0: Total Reward 1.00\n",
      "Round 100: Total Reward 1.00\n",
      "Round 200: Total Reward 1.00\n",
      "Round 300: Total Reward 1.00\n",
      "Round 400: Total Reward 1.00\n",
      "Round 500: Total Reward 1.00\n",
      "Round 600: Total Reward 1.00\n",
      "Round 700: Total Reward 1.00\n",
      "Round 800: Total Reward 1.00\n",
      "Round 900: Total Reward 1.00\n",
      "\n",
      "Final estimated values for each item (LCB values):\n",
      "[0.57762938 0.22203673 0.57453936 0.90243902 0.44067797]\n"
     ]
    }
   ],
   "source": [
    "n_items = 5 # Number of items (base arms)\n",
    "k = 3 # Length of the ranked list (number of items to recommend)\n",
    "n_rounds = 1000 # Number of rounds (iterations)\n",
    "# Simulated true reward function (probability of satisfaction for each item)\n",
    "true_rewards = np.random.rand(n_items)\n",
    "\n",
    "# Function to simulate the reward for each item (whether the user is satisfied or not)\n",
    "def get_reward(item_index):\n",
    "    # Simulate a Bernoulli feedback where reward is 1 (satisfied) with probability `true_rewards[item_index]`\n",
    "    return np.random.binomial(1, true_rewards[item_index])\n",
    "\n",
    "# Cascading Bandits using UCB\n",
    "class CascadingBandits:\n",
    "    def __init__(self, n_items, k):\n",
    "        self.n_items = n_items\n",
    "        self.k = k\n",
    "        self.counts = np.zeros(n_items)  # Number of times each item has been shown\n",
    "        self.total_rewards = np.zeros(n_items)  # Total rewards for each item\n",
    "        self.values = np.zeros(n_items)  # Estimated values (mean rewards) for each item\n",
    "        self.time = 0  # Total number of actions taken\n",
    "    def compute_lcb(self, item_index):\n",
    "        # Lower Confidence Bound (LCB) for the item\n",
    "        if self.counts[item_index] == 0:\n",
    "            return float('inf')  # If the item has never been shown, assign a high LCB\n",
    "        lcb = self.values[item_index] - np.sqrt(np.log(self.time + 1) / (2 * self.counts[item_index]))\n",
    "        return lcb\n",
    "    def select_action(self):\n",
    "        # Select the item with the lowest LCB (to reduce uncertainty)\n",
    "        lcb_values = [self.compute_lcb(i) for i in range(self.n_items)]\n",
    "        return np.argmin(lcb_values)\n",
    "    def update(self, shown_items, rewards):\n",
    "        # Update the counts, total rewards, and estimated values for the shown items\n",
    "        for item, reward in zip(shown_items, rewards):\n",
    "            self.counts[item] += 1\n",
    "            self.total_rewards[item] += reward\n",
    "            self.values[item] = self.total_rewards[item] / self.counts[item]\n",
    "        self.time += 1  # Increment the time step\n",
    "        \n",
    "    def train(self, rounds):\n",
    "        # Train the Cascading Bandits model using UCB and cascading feedback\n",
    "        for round in range(rounds):\n",
    "            # Randomly select k items to recommend (can be changed to a more sophisticated selection)\n",
    "            recommended_items = random.sample(range(self.n_items), self.k)\n",
    "            total_reward = 0\n",
    "            # Simulate cascading feedback (user examines items in the recommended order)\n",
    "            for item in recommended_items:\n",
    "                reward = get_reward(item)\n",
    "                total_reward += reward\n",
    "                if reward == 1:\n",
    "                    break # Stop at first satisfactory item\n",
    "            # Update the model based on the feedback\n",
    "            self.update(recommended_items, [get_reward(item) for item in recommended_items])\n",
    "            if round % 100 == 0:\n",
    "                print(f\"Round {round}: Total Reward {total_reward:.2f}\")\n",
    "\n",
    "# Initialize the Cascading Bandits model\n",
    "cascading_bandits = CascadingBandits(n_items, k)\n",
    "# Train the model with the given number of rounds\n",
    "cascading_bandits.train(n_rounds)\n",
    "# After training, display the learned values (UCB-based estimates of item rewards)\n",
    "print(\"\\nFinal estimated values for each item (LCB values):\")\n",
    "print(cascading_bandits.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f499b12d",
   "metadata": {},
   "source": [
    "1. Explanation:\n",
    "- **Simulated Environment**:\n",
    "   * The `true_rewards` array holds the satisfaction probabilities for each item. Each item has a true probability of satisfying the user when recommended.\n",
    "   * The `get_reward()` function simulates the feedback, where a reward of `1` means the user is satisfied, and `0` means they are not.\n",
    "\n",
    "- **Cascading Bandits Algorithm**:\n",
    "   * **UCB (Upper Confidence Bound)**: The `compute_lcb()` method computes the Lower Confidence Bound (LCB) for each item, which is a pessimistic estimate of the reward.\n",
    "   * **Item Selection**: The `select_action()` method selects the item with the lowest LCB value (most uncertain).\n",
    "   * **Update**: The `update()` method updates the statistics of the selected items based on the observed feedback.\n",
    "\n",
    "- **Training**: The `train()` method simulates the cascading feedback process for `n_rounds` iterations. In each round, a set of `k` items is selected, and the user's satisfaction with these items is observed in order. If the user is satisfied with an item, they stop interacting, and the subsequent items are ignored.\n",
    "\n",
    "- **Cascading Feedback**: In each round, the model selects `k` items. As the user interacts with the items in the order they are presented, the model updates based on the first item that satisfies the user (`reward == 1`).\n",
    "\n",
    "2. Output:\n",
    "- The model tracks the total reward for each round and updates its knowledge (estimated rewards) for each item. After all rounds, the learned values (LCBs) are displayed.\n",
    "\n",
    "- The code will print the total rewards accumulated during each round and, at the end, the estimated values for each item based on the UCB algorithm.\n",
    "\n",
    "3. Note:\n",
    "- This is a simplified `Cascading Bandit` and assumes **independent rewards** for each item.\n",
    "- In more complex settings, rewards could be dependent, then we can replace the random selection of items with greedy or Thompson Sampling selection strategy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:c182]",
   "language": "python",
   "name": "conda-env-c182-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
